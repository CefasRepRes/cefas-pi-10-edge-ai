#!/usr/bin/env python3

import argparse
import os
import tarfile  # Import the tarfile module
from azureml.core import Dataset
from azureml.core.compute import ComputeTarget
from azureml.data.dataset_factory import DataType
from azureml.core import Workspace, Datastore
import subprocess

def movedatasettoazurecomputeinstance(wsname, wssubscription_id, wsresource_group, dsname):
    ws = Workspace.get(name=wsname, subscription_id=wssubscription_id, resource_group=wsresource_group)
    print(ws)
    dataset = Dataset.get_by_name(ws, name=dsname)
    mount_context = dataset.download(target_path=dsname, overwrite=True)
    return ws, dataset

def get_datastore_path(ws, ds_name):
    datastore = Datastore.get(ws, ds_name)
    return datastore


def extract_tar(file_path, extract_to):
    """Extracts a .tar file to the specified directory."""
    with tarfile.open(file_path, 'r') as tar:
        tar.extractall(path=extract_to)
    print(f"Extracted {file_path} to {extract_to}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Plankton classifier")

    parser.add_argument(
        "-g",
        "--gray",
        action="store_true",
        help="load lighter weight(Resnet 18) with gray scale",
    )

    parser.add_argument(
        "-m",
        "--model_version",
        type=int,
        default=0,
        help="model version zero means Resnet50, and 1 means Resnet18, default=0",
    )

    parser.add_argument(
        "-b",
        "--batch_size",
        type=int,
        default=1,
        help="Batch size for processing images",
    )
    
    parser.add_argument(
        "-s",
        "--wsname",
        type=str,
        default="0",
        help="work space name",
    )

    parser.add_argument(
        "-t",
        "--wssubscription_id",
        type=str,
        default="0",
        help="work space subscription id",
    )
    
    parser.add_argument(
        "-u",
        "--wsresource_group",
        type=str,
        default="0",
        help="work space resource group",
    )
    
    parser.add_argument(
        "-v",
        "--dsname",
        type=str,
        default="0",
        help="data asset name",
    )    

    parser.add_argument(
        "-w",
        "--datastorename",
        type=str,
        default="0",
        help="data store name",
    )
    
    # Parse the command-line arguments
    args = parser.parse_args()
    print(f"Arguments parsed: {args}")
    # Move the dataset to the Azure Compute Instance
    print("Moving dataset to Azure Compute Instance...")
    ws, dataset = movedatasettoazurecomputeinstance(
        wsname=args.wsname,
        wssubscription_id=args.wssubscription_id,
        wsresource_group=args.wsresource_group,
        dsname=args.dsname
    )
    print(f"Workspace: {ws}")
    print(f"Dataset: {dataset}")
    # Set up the path for the downloaded dataset
    downloaded_directory = args.dsname
    print(f"Downloaded directory: {downloaded_directory}")
    # Construct the path for the .tar file
    tar_file_path = downloaded_directory+ "/"+args.dsname
    print(f"Path to .tar file: {tar_file_path}")
    # Define the extraction directory
    extract_dir = "./outputs/"
    print(f"Extraction directory: {extract_dir}")
    # Ensure the extraction directory exists
    if not os.path.exists(extract_dir):
        print(f"Extraction directory does not exist. Creating directory: {extract_dir}")
        os.makedirs(extract_dir)
    else:
        print(f"Extraction directory already exists: {extract_dir}")
    # Extract the .tar file
    print(f"Extracting .tar file from {tar_file_path} to {extract_dir}...")
    extract_tar(tar_file_path, extract_dir)
    print(f"Extraction completed. Files are now in {extract_dir}.")

    print(f"Deleting the dataset from the compute instance")
    os.remove(tar_file_path)
    print(f"Deleted. Uploading extraction...")
    result = subprocess.run(["azcopy", "login", "--identity"], capture_output=True, text=True)
    print("stdout:", result.stdout, "\nstderr:", result.stderr, "\nReturn code:", result.returncode)


#    # Upload to Azure Blob Storage
#    datastore = get_datastore_path(ws, args.datastorename)
#    blob_file_path = f"{args.dsname}/"
#    datastore.upload(extract_dir,target_path=blob_file_path,overwrite=True,show_progress=False)
#    print(f"Uploaded file to blob: {blob_file_path}")    




